<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="amazon-emr">
   <title>s3:// Protocol</title>
   <body>
      <p>The <codeph>s3</codeph> protocol is used in a URL that specifies the location of an Amazon
         Simple Storage Service (Amazon S3) bucket. Amazon S3 provides secure, durable,
         highly-scalable object storage. For information about Amazon S3, see <xref
            href="https://aws.amazon.com/s3/" format="html" scope="external">Amazon S3</xref>. </p>
      <p>Before creating an external table with the s3 protocol you must configure Greenplum
         Database. See <xref href="#amazon-emr/s3_prereq" format="dita"/>.</p>
      <p>For the <codeph>s3</codeph> protocol, you specify a location for files and an optional
         configuration file in the <codeph>LOCATION</codeph> clause of the <codeph>CREATE EXTERNAL
            TABLE</codeph> command. This is the syntax:</p>
      <codeblock>'s3://<varname>S3_endpoint</varname>/<varname>bucket_name</varname>/[<varname>S3_prefix</varname>] [config=<varname>config_file_location</varname>]'</codeblock>
      <p>The <codeph>s3</codeph> protocol URL specifies the AWS S3 endpoint and S3 bucket name. Each
         Greenplum Database segment instance must have access to the S3 location.</p>
      <p>For writeable S3 tables, the <codeph>s3</codeph> protocol URL specifies the endpoint and
         bucket name where data files are uploaded for the table. The S3 file prefix is used when
         creating new files in the S3 location as a result of inserting data to the table. See <xref
            href="#amazon-emr/section_c2f_zvs_3x" format="dita"/>.</p>
      <p>For read-only S3 tables, the S3 file prefix is optional. If you specify an
            <varname>S3_prefix</varname> for read-only s3 tables, the <codeph>s3</codeph> protocol
         selects those files that have the specified S3 file prefix. The <codeph>s3</codeph>
         protocol does not use the slash character (<codeph>/</codeph>) as delimiter. For example,
         consider the following 5 files that each have <codeph>domain</codeph> as the
            <varname>S3_endpoint</varname>, and <codeph>test1</codeph> as the
            <varname>bucket_name</varname>:</p>
      <codeblock>s3://domain/test1/abc
s3://domain/test1/abc/
s3://domain/test1/abc/xx
s3://domain/test1/abcdef
s3://domain/test1/abcdefff</codeblock>
      <ul id="ul_yll_xjm_qv">
         <li>If the S3 URL is provided as <codeph>s3://domain/test1/abc</codeph>, then the
               <codeph>s3</codeph> protocol selects all 5 files.</li>
         <li>If the S3 URL is provided as <codeph>s3://domain/test1/abc/</codeph>, then the
               <codeph>s3</codeph> protocol selects the files
               <codeph>s3://domain/test1/abc/</codeph> and
            <codeph>s3://domain/test1/abc/xx</codeph>.</li>
         <li>If the S3 URL is provided as <codeph>s3://domain/test1/abcd</codeph>, then the
               <codeph>s3</codeph> protocol selects the files
               <codeph>s3://domain/test1/abcdef</codeph> and
               <codeph>s3://domain/test1/abcdefff</codeph></li>
      </ul>
      <p>Wildcard characters are not supported in a <varname>S3_prefix</varname>. </p>
      <p>For information about the AWS S3 endpoints see <xref
            href="http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region" format="html"
            scope="external"
            >http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region</xref>. For
         information about S3 buckets and folders, see the Amazon S3 documentation <xref
            href="http://aws.amazon.com/documentation/s3/" format="html" scope="external"
            >http://aws.amazon.com/documentation/s3/</xref>. For information about the S3 file
         prefix, see the Amazon S3 documentation <xref
            href="http://docs.aws.amazon.com/AmazonS3/latest/dev/ListingKeysHierarchy.html"
            format="html" scope="external">Listing Keys Hierarchically Using a Prefix and
            Delimiter</xref>.</p>
      <p>The <codeph>config</codeph> parameter specifies the location of the required
            <codeph>s3</codeph> protocol configuration file that contains AWS connection credentials
         and communication parameters. See <xref href="#amazon-emr/s3_config_param" format="dita"
         />.</p>
      <section id="section_c2f_zvs_3x">
         <title>About S3 Data Files</title>
         <p>For writeable S3 tables, an insert operation results in one or more files placed in the
            S3 bucket using the filename format <codeph>
               &lt;prefix>&lt;sha>&lt;segment_id>.&lt;extension>[.gz]</codeph> where:<ul
               id="ul_sw1_qvs_3x">
               <li><codeph>&lt;prefix></codeph> is the prefix specified in the S3 URL.</li>
               <li><codeph>&lt;sha></codeph> is a SHA256 digest that is used to ensure a unique
                  filename.</li>
               <li><codeph>&lt;segment_id></codeph> is the Greenplum Database segment ID.</li>
               <li><codeph>&lt;extension></codeph> describes the file type (<codeph>.txt</codeph> or
                     <filepath>.csv</filepath>, depending on the value you provide in the
                     <codeph>FORMAT</codeph> clause of <codeph>CREATE EXTERNAL TABLE</codeph>).
                  Files created by the <codeph>gpcheckcloud</codeph> utility always uses the
                  extension <filepath>.data</filepath>.</li>
               <li><filepath>.gz</filepath> is appended to the filename if compression is enabled
                  for S3 writeable tables (the default).</li>
            </ul></p>
         <p>For writeable S3 tables, the number of files created for insert operations can be
            configured using the <codeph>chunksize</codeph> configuration parameter described in
               <xref href="#amazon-emr/s3_config_file" format="dita"/>.</p>
         <p>For read-only S3 tables, all of the files specified by the S3 file location
               (<varname>S3_endpoint</varname>/<varname>bucket_name</varname>/<varname>S3_prefix</varname>)
            are used as the source for the external table and must have the same format. Each file
            must also contain complete data rows. A data row cannot be split between files. Only the
               <codeph>TEXT</codeph> and <codeph>CSV</codeph> formats are supported. The files can
            be in gzip compressed format. The <codeph>s3</codeph> protocol recognizes the gzip
            format and uncompress the files. Only the gzip compression format is supported. </p>
         <p>The S3 file permissions must be <codeph>Open/Download</codeph> and <codeph>View</codeph>
            for the S3 user ID that is accessing the files.</p>
         <p>For read-only S3 tables, each segment can download one file at a time from S3 location
            using several threads. To take advantage of the parallel processing performed by the
            Greenplum Database segments, the files in the S3 location should be similar in size and
            the number of files should allow for multiple segments to download the data from the S3
            location. For example, if the Greenplum Database system consists of 16 segments and
            there was sufficient network bandwidth, creating 16 files in the S3 location allows each
            segment to download a file from the S3 location. In contrast, if the location contained
            only 1 or 2 files, only 1 or 2 segments download data.</p>
      </section>
      <section id="s3_config_param">
         <title>About the S3 Protocol config Parameter</title>
         <p>The optional <codeph>config</codeph> parameter specifies the location of the required
               <codeph>s3</codeph> protocol configuration file. The file contains AWS connection
            credentials and communication parameters. For information about the file, see <xref
               href="#amazon-emr/s3_config_file" format="dita"/>.</p>
         <p>The configuration file is required on all Greenplum Database segment hosts. This is
            default location is a location in the data directory of each Greenplum Database segment
            instance.<codeblock><varname>gpseg_data_dir</varname>/<varname>gpseg_prefix</varname><varname>N</varname>/s3/s3.conf</codeblock></p>
         <p>The <varname>gpseg_data_dir</varname> is the path to the Greenplum Database segment data
            directory, the <varname>gpseg_prefix</varname> is the segment prefix, and
               <varname>N</varname> is the segment ID. The segment data directory, prefix, and ID
            are set when you initialize a Greenplum Database system.</p>
         <p>If you have multiple segment instances on segment hosts, you can simplify the
            configuration by creating a single location on each segment host. Then you specify the
            absolute path to the location with the <codeph>config</codeph> parameter in the
               <codeph>s3</codeph> protocol <codeph>LOCATION</codeph> clause. This example specifies
            a location in the <codeph>gpadmin</codeph> home directory. </p>
         <codeblock>LOCATION ('s3://s3.amazonaws.com/test/my_data config=/home/gpadmin/s3.conf')</codeblock>
         <p>All segment instances on the hosts use the file
               <codeph>/home/gpadmin/s3/s3.conf</codeph>.</p>
      </section>
      <section id="s3_prereq"><title>s3 Protocol Prerequisites</title><p>Before you create a an
            external table with the <codeph>s3</codeph> protocol, you must configure the Greenplum
            Database system:<ul id="ul_qlk_42s_55">
               <li>Configure the database to support the <codeph>s3</codeph> protocol.</li>
               <li>Create and install the <codeph>s3</codeph> protocol configuration file on all the
                  Greenplum Database segments.</li>
            </ul></p><p><b>To configure a database to support the <codeph>s3</codeph>
            protocol</b></p><ol id="ol_ptx_bfs_55">
            <li>Create a function to access the <codeph>s3</codeph> protocol library.<p>In each
                  Greenplum database that accesses an S3 bucket with the <codeph>s3</codeph>
                  protocol, create a function for the protocol. For writeable S3
                  tables:<codeblock>CREATE OR REPLACE FUNCTION write_to_s3() RETURNS integer AS
   '$libdir/gps3ext.so', 's3_export' LANGUAGE C STABLE;</codeblock></p><p>For
                  read-only S3
               tables:</p><codeblock>CREATE OR REPLACE FUNCTION read_from_s3() RETURNS integer AS
   '$libdir/gps3ext.so', 's3_import' LANGUAGE C STABLE;</codeblock></li>
            <li> Declare the <codeph>s3</codeph> protocol and specify the function that is used to
               write to or read from an S3 bucket. For writeable S3
                  tables:<codeblock>CREATE PROTOCOL s3 (writefunc  = write_to_s3);</codeblock><p>For
                  read-only S3
                  tables:<codeblock>CREATE PROTOCOL s3 (readfunc = read_from_s3);</codeblock></p></li>
         </ol><note>The protocol name <codeph>s3</codeph> must be the same as the protocol of the
            URL specified for the external table you create to access an S3 resource. <p>The
               corresponding function is called by every Greenplum Database segment instance. All
               segment hosts must have access to the S3 bucket.</p></note><b>To create and install
            the <codeph>s3</codeph> protocol configuration file</b><ol id="ol_c1r_nqt_55">
            <li>Create a configuration file with the S3 configuration information:<ol
                  id="ol_kq2_5bt_3x">
                  <li>Create a template file using the <codeph>gpcloudcheck</codeph>
                     utility:<codeblock>gpcheckcloud -t > ./mytest_s3.config</codeblock></li>
                  <li>Edit the template file to specify the accessid and secret required to connect
                     to the S3 location.</li>
                  <li>Edit any other configuration file parameters as described in <xref
                        href="#amazon-emr/s3_config_file" format="dita"/>.</li>
               </ol></li>
            <li>Copy the file to the same location for all Greenplum Database segments on all
                  hosts.<p>The default location is
                        <codeph><varname>gpseg_data_dir</varname>/<varname>gpseg_prefix</varname><varname>N</varname>/s3/s3.conf</codeph>.
                  If you can install the file in a different location, you must specify the location
                  with the <codeph>config</codeph> parameter in the <codeph>s3</codeph> protocol
                  URL. See <xref href="#amazon-emr/s3_config_param" format="dita"/>.</p></li>
         </ol></section>
      <section id="s3_config_file">
         <title>s3 Protocol Configuration File</title>
         <p>When using the <codeph>s3</codeph> protocol, the <codeph>s3</codeph> protocol
            configuration file is required on all Greenplum Database segments. The default location
            is
            <codeblock><varname>gpseg_data_dir</varname>/<varname>gpseg-prefix</varname><varname>N</varname>/s3/s3.conf</codeblock></p>
         <p>The <varname>gpseg_data_dir</varname> is the path to the Greenplum Database segment data
            directory, the <varname>gpseg-prefix</varname> is the segment prefix, and
               <varname>N</varname> is the segment ID. The segment data directory, prefix, and ID
            are set when you initialize a Greenplum Database system.</p>
         <p>If you have multiple segment instances on segment hosts, you can simplify the
            configuration by creating a single location on each segment host. Then you specify the
            absolute path to the location with the <codeph>config</codeph> parameter in the
               <codeph>s3</codeph> protocol <codeph>LOCATION</codeph> clause. This example specifies
            a location in the <codeph>gpadmin</codeph> home directory. </p>
         <codeblock>config=/home/gpadmin/s3/s3.conf</codeblock>
         <p>All segment instances on the hosts use the file
               <codeph>/home/gpadmin/s3/s3.conf</codeph>.</p>
         <p>The <codeph>s3</codeph> protocol configuration file is a text file that consists of a
               <codeph>[default]</codeph> section and parameters This is an example configuration
            file.<codeblock>[default]
secret = "secret"
accessid = "user access id"
connections = 3
chunksize = 67108864</codeblock></p>
         <p>You can use the Greenplum Database <codeph>gpcheckcloud</codeph> utility to test the S3
            configuration file. See <xref href="#amazon-emr/s3chkcfg_utility" format="dita"/>.</p>
         <sectiondiv>
            <p><b>s3 Configuration File Parameters</b></p>
            <parml>
               <plentry>
                  <pt>accessid</pt>
                  <pd>Required. AWS S3 ID to access the S3 bucket.</pd>
               </plentry>
               <plentry>
                  <pt>secret</pt>
                  <pd>Required. AWS S3 passcode for the S3 ID to access the S3 bucket.</pd>
               </plentry>
               <plentry>
                  <pt>autocompress</pt>
                  <pd>For writeable S3 external tables, this parameter specifies whether to compress
                     files (using gzip) before uploading to S3. Files are compressed by default if
                     you do not specify this parameter.</pd>
               </plentry>
               <plentry>
                  <pt>chunksize</pt>
                  <pd>The buffer size that each segment thread uses for reading from or writing to
                     the S3 server. The default is 64 MB. The minimum is 8MB and the maximum is
                     128MB. When inserting data to an S3 table, each Greenplum Database segment
                     writes the data into its buffer until it is full, after which it writes the
                     buffer to a file in the S3 bucket. This process is then repeated as necessary
                     on each segment until the insert operation completes.</pd>
               </plentry>
               <plentry>
                  <pt>threadnum</pt>
                  <pd>The maximum number of concurrent connections a segment can create when
                     downloading data from the S3 bucket. The default is 4. The minimum is 1 and the
                     maximum is 8.</pd>
               </plentry>
               <plentry>
                  <pt>encryption</pt>
                  <pd>Use connections that are secured with Secure Sockets Layer (SSL). Default
                     value is <codeph>true</codeph>. The values <codeph>true</codeph>,
                        <codeph>t</codeph>, <codeph>on</codeph>, <codeph>yes</codeph>, and
                        <codeph>y</codeph> (case insensitive) are treated as <codeph>true</codeph>.
                     Any other value is treated as <codeph>false</codeph>.</pd>
               </plentry>
               <plentry>
                  <pt>low_speed_limit</pt>
                  <pd>The download speed lower limit, in bytes per second. The default speed is
                     10240 (10K). If the download speed is slower than the limit for longer than the
                     time specified by <codeph>low_speed_time</codeph>, the download connection is
                     aborted and retried. After 3 retries, the <codeph>s3</codeph> protocol returns
                     an error. A value of 0 specifies no lower limit.</pd>
               </plentry>
               <plentry>
                  <pt>low_speed_time</pt>
                  <pd>When the connection speed is less than <codeph>low_speed_limit</codeph>, the
                     amount of time, in minutes, to wait before aborting a download from an S3
                     bucket. The default is 1 minute. A value of 0 specifies no time limit.</pd>
               </plentry>
            </parml>
            <note>You must ensure that there is sufficient memory on the Greenplum Database segment
               hosts when the <codeph>s3</codeph> protocol to accesses the files. Greenplum Database
               allocates <codeph>connections * chunksize</codeph> memory on each segment host when
               accessing S3 files.</note>
         </sectiondiv>
      </section>
      <section>
         <title>s3 Protocol Limitations</title>
         <p>These are <codeph>s3</codeph> protocol limitations: <ul id="ul_xt5_4cz_55">
               <li>Only the S3 path-style URL is
                  supported.<codeblock>s3://<varname>S3_endpoint</varname>/<varname>bucketname</varname>/[<varname>S3_prefix</varname>]</codeblock></li>
            </ul><ul id="ul_qqg_qcz_55">
               <li>Only the S3 endpoint is supported. The protocol does not support virtual hosting
                  of S3 buckets (binding a domain name to an S3 bucket).</li>
               <li>AWS signature version 2 and version 4 signing process are supported. <p>For
                     information about the S3 endpoints supported by each signing process, see <xref
                        href="http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region"
                        format="html" scope="external"
                        >http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region</xref>.</p></li>
               <li>S3 encryption is not supported. The S3 file property <uicontrol>Server Side
                     Encryption</uicontrol> must be <codeph>None</codeph>.</li>
               <li>For writeable S3 external tables, only the <codeph>INSERT</codeph> operation is
                  supported. <codeph>UPDATE</codeph>, <codeph>DELETE</codeph>, and
                     <codeph>TRUNCATE</codeph> operations are not supported.</li>
            </ul></p>
      </section>
      <section id="s3chkcfg_utility"><title>About the gpcheckcloud Utility</title><p>The Greenplum
            Database utility <codeph>gpcheckcloud</codeph> helps users create an <codeph>s3</codeph>
            protocol configuration file and test a configuration file. You can specify options to
            test the ability to access an S3 bucket with a configuration file, and optionally
            download data from files in the bucket.</p><p>If you run the utility without any
            options, it sends a template configuration file to <codeph>STDOUT</codeph>. You can
            capture the output and create an <codeph>s3</codeph> configuration file to connect to
            Amazon S3. </p><p>The utility is installed in the Greenplum Database
               <codeph>$GPHOME/bin</codeph> directory.</p><b>Syntax</b>
         <codeblock>gpcheckcloud {<b>-c</b> | <b>-d</b>} "<b>s3://</b><varname>S3_endpoint</varname>/<varname>bucketname</varname>/[<varname>S3_prefix</varname>] [config==<varname>path_to_config_file</varname>]"

gpcheckcloud <b>-u</b> &lt;file_to_upload> "<b>s3://</b><varname>S3_endpoint</varname>/<varname>bucketname</varname>/[<varname>S3_prefix</varname>] [config==<varname>path_to_config_file</varname>]"
gpcheckcloud <b>-t</b>

gpcheckcloud <b>-h</b></codeblock>
         <b>Options</b>
         <parml>
            <plentry>
               <pt>-c</pt>
               <pd>Connect to the specified S3 location with the configuration specified in the
                     <codeph>s3</codeph> protocol URL and return information about the files in the
                  S3 location.</pd>
               <pd>If the connection fails, the utility displays information about failures such as
                  invalid credentials, prefix, or server address (DNS error), or server not
                  available.</pd>
            </plentry>
            <plentry>
               <pt>-d</pt>
               <pd>Download data from the specified S3 location with the configuration specified in
                  the <codeph>s3</codeph> protocol URL and send the output to
                     <codeph>STDOUT</codeph>.</pd>
               <pd>If files are gzip compressed, the uncompressed data is sent to
                     <codeph>STDOUT</codeph>.</pd>
            </plentry>
            <plentry>
               <pt>-u</pt>
               <pd>Upload a file to the S3 bucket specified in the <codeph>s3</codeph> protocol URL
                  using the specified configuration file if available. Use this option to test
                  compression and chunksize setting for your configuration.</pd>
            </plentry>
            <plentry>
               <pt>-t</pt>
               <pd>Sends a template configuration file to <codeph>STDOUT</codeph>. You can capture
                  the output and create an <codeph>s3</codeph> configuration file to connect to
                  Amazon S3. </pd>
            </plentry>
            <plentry>
               <pt>-h</pt>
               <pd>Display <codeph>gpcheckcloud</codeph> help.</pd>
            </plentry>
         </parml><p><b>Examples</b>
         </p><p>This example runs the utility without options to create a template
               <codeph>s3</codeph> configuration file <codeph>mytest_s3.config</codeph> in the
            current directory.<codeblock>gpcheckcloud -t > ./mytest_s3.config</codeblock></p><p>This
            example attempts to upload a local file, <filepath>test-data.csv</filepath> to an S3
            bucket location using the <codeph>s3</codeph> configuration file
               <codeph>s3.mytestconf</codeph>:<codeblock>gpcheckcloud -u ./test-data.csv "s3://domain/test1/abc config=s3.mytestconf"</codeblock></p><p>A
            successful upload results in one or more files placed in the S3 bucket using the
            filename format <codeph> &lt;prefix>&lt;sha>&lt;segment_id>.data[.gz]</codeph>. See
               <xref href="#amazon-emr/section_c2f_zvs_3x" format="dita"/>.</p><p>This example
            attempts to connect to an S3 bucket location with the <codeph>s3</codeph> configuration
            file
            <codeph>s3.mytestconf</codeph>.<codeblock>gpcheckcloud -c "s3://domain/test1/abc config=s3.mytestconf"</codeblock></p><p>Download
            all files from the S3 bucket location and send the output to <codeph>STDOUT</codeph>.
            <codeblock>gpcheckcloud -d "s3://domain/test1/abc config=s3.mytestconf"</codeblock></p></section>
   </body>
</topic>